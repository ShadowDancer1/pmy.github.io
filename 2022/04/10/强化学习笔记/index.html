<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo_icon001.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo_icon001.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="记录自己强化学习学习过程">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记">
<meta property="og:url" content="http://example.com/2022/04/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="动态">
<meta property="og:description" content="记录自己强化学习学习过程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/04/11/VeLkxvf93CpXI4l.jpg">
<meta property="og:image" content="https://s2.loli.net/2022/04/11/sG1vbJLijYZStgD.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/11/Y8dkctwnWX3xJOy.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/APTFzEkv1ZMiGQx.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/SOyXw3Vgd7WCtBk.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/zJUQEgT1XStOHxf.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/riFPYbIwQ7jLWfm.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/kEdlX1IupJPsroq.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/IYFwo8DcpELsXnP.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/14/15EzaUgscWky89t.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/14/narPG5kOxpNVwD9.png">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2022/04/14/371ae56e60eaa013.png">
<meta property="og:image" content="https://s1.ax1x.com/2022/04/14/L1kh1P.png">
<meta property="og:image" content="https://img30.360buyimg.com/pop/jfs/t1/151963/30/22549/983753/6257f23eEba2269cf/141f1bcf9aa79f54.png">
<meta property="og:image" content="https://img30.360buyimg.com/pop/jfs/t1/214725/25/17102/632112/6257f21bE89ca862a/774c8fe0fdd93b77.png">
<meta property="og:image" content="https://img30.360buyimg.com/pop/jfs/t1/188468/40/23237/1878038/6257f051Ef267260a/0b049abbd92a7cce.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/u3MjVKWYFRAIiTb.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/Yte1AsnRpvOm3bw.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/pZjodmA9li34EIV.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/8mg9yo1sVQJuDbk.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/M78EQ53ghfvyTO2.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/17/ObnDNdgfz4HYpcC.png">
<meta property="article:published_time" content="2022-04-10T09:18:13.000Z">
<meta property="article:modified_time" content="2022-04-17T10:46:19.176Z">
<meta property="article:author" content="P.M.Y">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/04/11/VeLkxvf93CpXI4l.jpg">

<link rel="canonical" href="http://example.com/2022/04/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习笔记 | 动态</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3be792f493e893537d1d13a6c8b3b054";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">动态</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/creator.jpg">
      <meta itemprop="name" content="P.M.Y">
      <meta itemprop="description" content="记录·分享·进步">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="动态">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">

	    

              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-10 17:18:13" itemprop="dateCreated datePublished" datetime="2022-04-10T17:18:13+08:00">2022-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-17 18:46:19" itemprop="dateModified" datetime="2022-04-17T18:46:19+08:00">2022-04-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="https://s2.loli.net/2022/04/11/VeLkxvf93CpXI4l.jpg" alt="ismail-inceoglu-cauligrx"></p>
<p>记录自己强化学习学习过程</p>
<span id="more"></span>
<h3 id="4月11日"><a href="#4月11日" class="headerlink" title="4月11日"></a>4月11日</h3><ol>
<li><p>reinforcement learning算法的本质就是让agent获得状态转移矩阵</p>
</li>
<li><p>监督学习与强化学习的区别：</p>
<p>监督学习——Learning from teacher</p>
<p>强化学习——Learning from experience</p>
<p>Alpha Go 先进行监督学习，获得不错的效果之后再进行强化学习。</p>
</li>
<li><p>强化学习的困难之处</p>
<p>1）Reward delay</p>
<p>2）Agent的行为会对之后的情况产生影响（需要Agent具备探索能力）</p>
</li>
<li><p>Machine Learning ≈ Looking for a Function</p>
<p>在强化学习中，这里的Function相当于Action = Π(Observation)</p>
</li>
<li><p>强化学习方法</p>
<p>1）model-based(针对未来情况进行预测)</p>
<p>2）model-free (policy-based and/or value-based)</p>
<p>​    例：Alpha Go采用的是 policy-based+value-based+model-based</p>
</li>
<li><p>在一场游戏里面，我们把环境输出的 s 跟演员输出的行为 a ，把 s  跟 a  全部串起来， 叫做一个 <code>Trajectory(轨迹)</code>，如下式所示。</p>
<script type="math/tex; mode=display">
\text { Trajectory } \tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}</script></li>
<li><p>你可以计算每一个轨迹发生的概率。假设现在演员的参数已经被给定了话，就是 $\theta$。根据 $\theta$，你其实可以计算某一个轨迹发生的概率，你可以计算某一个回合里面发生这样子状况的概率。</p>
<script type="math/tex; mode=display">
\begin{aligned} p_{\theta}(\tau) &=p\left(s_{1}\right) p_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) p_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots \ \\ &=p\left(s_{1}\right) \prod_{t=1}^{T} p_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right) \end{aligned}</script></li>
<li><p>你可以根据 $\theta$ 算出某一个轨迹 $\tau$ 出现的概率，接下来计算这个 $\tau$ 的总奖励是多少。总奖励使用这个 $\tau$ 出现的概率进行加权，对所有的 $\tau$ 进行求和，就是期望值。给定一个参数，你会得到的期望值。</p>
<script type="math/tex; mode=display">
\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=E_{\tau \sim p_{\theta}(\tau)}[R(\tau)]</script></li>
<li><p>计算梯度</p>
<script type="math/tex; mode=display">
\nabla \bar{R}_{\theta}=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script></li>
<li><p>强化学习的一下Tips</p>
<p>1）添加baseline</p>
<script type="math/tex; mode=display">
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right)-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right) \\ b \approx E[R(\tau)]</script><p>2）给每个动作合适的分数</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2022/04/11/sG1vbJLijYZStgD.png" alt="4.17"></p>
<p>​    本来的权重是整场游戏的奖励的总和，现在改成从某个时间 t 开始，假设这个动作是在 t 这个时间点所执行       的，从 t 这个时间点一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的。</p>
<p>​    3）折扣回报</p>
<p>​    4）优势函数</p>
<p>​    把 R-b 这一项合起来，我们统称为<code>优势函数(advantage function)</code>， 用 <code>A</code> 来代表优势函数。优势函数取决于 s 和 a，我们就是要计算的是在某一个状态 s 采取某一个动作 a 的时候，优势函数有多大。</p>
<p><img src="https://s2.loli.net/2022/04/11/Y8dkctwnWX3xJOy.png" alt="4.19"></p>
<p>优势函数的意义在于，假设我们在某一个状态 $s_t$  执行某一个动作 $a_t$，相较于其他可能的动作，它有多好。它在意的不是一个绝对的好，而是相对的好，即<code>相对优势(relative advantage)</code>。因为会减掉一个 b，减掉一个 基线， 所以这个东西是相对的好，不是绝对的好。 $A^{\theta}\left(s_{t}, a_{t}\right)$ 通常可以是由一个网络估计出来的，这个网络叫做 critic。</p>
<hr>
<h3 id="4月12日"><a href="#4月12日" class="headerlink" title="4月12日"></a>4月12日</h3><ol>
<li><p>Behavior Cloning的缺点在于无法告诉Machine哪些是重要的，哪些是不重要的。</p>
</li>
<li><p>Two Learning Scenarios:</p>
<p>1）强化学习</p>
<p>2）Learning by demonstration</p>
</li>
<li><p>Actor-Critic算法：</p>
<p>1）Training an Actor</p>
<p>2）Training a Critic</p>
<p>3）Actor + Critic</p>
</li>
<li><p>一个 Critic 无法决定 Action，Critic 的作用是评估一个 actor Π 好不好。（给出最终奖励的期望）</p>
<p>评论家(Critic)是指值函数 $V^{\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。</p>
</li>
</ol>
<hr>
<h3 id="4月13日"><a href="#4月13日" class="headerlink" title="4月13日"></a>4月13日</h3><ol>
<li><p>DQN算法学习</p>
<p>1）有一种 Critic 是 <code>state value function(状态价值函数)</code>。评论家的输出是跟演员有关的，状态的价值其实取      决于你的演员，当演员变的时候，状态价值函数的输出其实也是会跟着改变的。</p>
<p>2）衡量状态价值函数 $V^{\pi}(s)$ 的两种方法：</p>
<p>​      （1）Monte-Carlo(MC)-based</p>
<p>​                在训练的时候， 它就是一个回归问题。网络的输出就是一个值，你希望在输入 $s_a$ 的时候，输出                 的值跟 $G_a$ 越近越好，输入 $s_b$ 的时候，输出的值跟 $G_b$ 越近越好。</p>
<p><img src="https://s2.loli.net/2022/04/13/APTFzEkv1ZMiGQx.png" alt=""></p>
<p>​      （2）Temporal-difference(时序差分)</p>
<p>​                TD-based 的方法不需要把游戏玩到底，只要在游戏的某一个情况，某一个状态 $s_t$ 的时候，采取动                作 $a_t$ 得到奖励$r_t$ ，跳到状态 $s_{t+1}$，就可以使用 TD 的方法。</p>
<p>​                <script type="math/tex">V^{\pi}\left(s_{t}\right)=V^{\pi}\left(s_{t+1}\right)+r_{t}</script></p>
<p><img src="https://s2.loli.net/2022/04/13/SOyXw3Vgd7WCtBk.png" alt=""></p>
<p>3）另一种 Ctitic 是 Q-function，也叫<code>state-action value function(状态-动作价值函数)</code>。状态-动作价值        函数的输入是一个状态、动作对，它的意思是说，在某一个状态采取某一个动作，假设我们都使用演员         $\pi$ ，得到的累积奖励的期望值有多大。</p>
<p>​       推导可得：</p>
<script type="math/tex; mode=display">
\begin{aligned} V^{\pi}(s) &\le Q^{\pi}(s,\pi'(s)) \ \\ &=E\left[r_{t}+V^{\pi}\left(s_{t+1}\right) | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right]\ \\&\le E\left[r_{t}+Q^{\pi}\left(s_{t+1}, \pi^{\prime}\left(s_{t+1}\right)\right) | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\&=E\left[r_{t}+r_{t+1}+V^{\pi}\left(s_{t+2}\right) |s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & \le E\left[r_{t}+r_{t+1}+Q^{\pi}\left(s_{t+2},\pi'(s_{t+2}\right) | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & = E\left[r_{t}+r_{t+1}+r_{t+2}+V^{\pi}\left(s_{t+3}\right) |s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & \le \cdots\ \\ & \le E\left[r_{t}+r_{t+1}+r_{t+2}+\cdots | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & = V^{\pi'}(s) \end{aligned}</script><p>​        因此：</p>
<p>​        <script type="math/tex">V^{\pi}(s)\le V^{\pi'}(s)</script></p>
</li>
</ol>
<p>​        4）DQN中用到的一些Tips:</p>
<p>​              （1）目标网络</p>
<p><img src="https://s2.loli.net/2022/04/13/zJUQEgT1XStOHxf.png" alt=""></p>
<p>​            （2）探索(Exploration)</p>
<p><img src="https://s2.loli.net/2022/04/13/riFPYbIwQ7jLWfm.png" alt=""></p>
<p>​            （3）Experience Replay(经验回放)</p>
<p>​                     构建一个 <code>Replay Buffer</code>，用先有某一个策略 $\pi$ 去跟环境做互动，然后它会去收集数据，并利用收                    集到的数据更新参数。</p>
<p><img src="https://s2.loli.net/2022/04/13/kEdlX1IupJPsroq.png" alt="6.18"></p>
<p>​    5）DQN算法总结</p>
<p>​          <img src="https://s2.loli.net/2022/04/13/IYFwo8DcpELsXnP.png" alt="6.19"></p>
<hr>
<h3 id="4月14日"><a href="#4月14日" class="headerlink" title="4月14日"></a>4月14日</h3><ol>
<li><p>DQN进阶技巧</p>
<p>1）Double DQN</p>
<p><img src="https://s2.loli.net/2022/04/14/15EzaUgscWky89t.png" alt=""></p>
<p>2）Dueling DQN</p>
<p><img src="https://s2.loli.net/2022/04/14/narPG5kOxpNVwD9.png" alt="7.6"></p>
<p>3）Prioritized Experience Replay</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/04/14/371ae56e60eaa013.png" alt=""></p>
<p>4）Balance between MC and TD</p>
<p><img src="https://s1.ax1x.com/2022/04/14/L1kh1P.png" alt="L1kh1P.png"></p>
<p>5）Noisy Net</p>
<p><img src="https://img30.360buyimg.com/pop/jfs/t1/151963/30/22549/983753/6257f23eEba2269cf/141f1bcf9aa79f54.png" alt="7.10.png"></p>
<p>6）Distributional Q-function</p>
<p><img src="https://img30.360buyimg.com/pop/jfs/t1/214725/25/17102/632112/6257f21bE89ca862a/774c8fe0fdd93b77.png" alt="7.12.png"></p>
<p>7）Rainbow</p>
<p><img src="https://img30.360buyimg.com/pop/jfs/t1/188468/40/23237/1878038/6257f051Ef267260a/0b049abbd92a7cce.png" alt="7.13.png"></p>
</li>
</ol>
<hr>
<h3 id="4月15日"><a href="#4月15日" class="headerlink" title="4月15日"></a>4月15日</h3><p>实现 DQN算法 玩 MountainCar-v0，并能够保存模型和加载模型。</p>
<p><img src="https://s2.loli.net/2022/04/16/u3MjVKWYFRAIiTb.png" alt=""></p>
<hr>
<h3 id="4月16日"><a href="#4月16日" class="headerlink" title="4月16日"></a>4月16日</h3><ol>
<li><p>PPO算法学习</p>
<p>1）on-policy 和 off-policy</p>
</li>
</ol>
<p>​     如果要学习的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做<code>on-policy(同策略)</code>。</p>
<p>​     如果要学习的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做<code>off-policy(异策略)</code>。</p>
<p> 2）policy gradient 是一个 on-policy 的算法。</p>
<p> 3）<code>近端策略优化(Proximal Policy Optimization，简称 PPO)</code> 是 policy gradient 的一个变形，它是现      在  OpenAI 默认的强化学习算法。</p>
<script type="math/tex; mode=display">
\nabla \bar{R}{\theta}=E{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right]</script><p> 4）重要性采样(Importance Sampling，IS)</p>
<script type="math/tex; mode=display">
E_{x \sim p}[f(x)] \approx \frac{1}{N} \sum_{i=1}^N f(x^i)</script><p>​      期望值 $E_{x \sim p}[f(x)]$ 其实就是 $\int f(x) p(x) dx$，对其做如下的变换：</p>
<script type="math/tex; mode=display">
\int f(x) p(x) d x=\int f(x) \frac{p(x)}{q(x)} q(x) d x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]</script><p>​      就可以写成对 q 里面所采样出来的 $x$ 取期望值，其中 $\frac{p(x)}{q(x)}$ 为<code>重要性权重(importance weight)</code> 。</p>
<p>​      在实现上，p 和 q 不能差太多。因为方差不同：</p>
<script type="math/tex; mode=display">
\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \\
\begin{aligned} \operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2} \ \\ &=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \end{aligned}</script><p><img src="https://s2.loli.net/2022/04/16/Yte1AsnRpvOm3bw.png" alt="5.4"></p>
<p> 5）on-policy  —-&gt;  off-policy</p>
<p><img src="https://s2.loli.net/2022/04/16/pZjodmA9li34EIV.png" alt="5.6"></p>
<script type="math/tex; mode=display">
E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
 =E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(s_{t}, a_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}, a_{t}\right)} A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
 =E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} \frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
 =E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]</script><p>  现在我们得到一个新的目标函数:</p>
<script type="math/tex; mode=display">
J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]</script><p>6）<strong>PPO 是 on-policy 的算法</strong>，<strong>在 PPO 中 $\theta’$ 是 $\theta_{\text{old}}$，即 behavior policy 也是 $\theta$</strong>。如果 $p_{\theta}\left(a_{t} | s_{t}\right)$ 跟 $p_{\theta’}\left(a_{t} | s_{t}\right)$ 这两个分布差太多的话，重要性采样的结果就会不好。这也是PPO要解决的问题。</p>
<p>7）在做 PPO 的时候，所谓的 KL 散度并不是参数的距离，而是动作的距离。</p>
<p>8）<strong>PPO-Penalty</strong></p>
<p>​                    <img src="https://s2.loli.net/2022/04/16/8mg9yo1sVQJuDbk.png" alt="5.9"></p>
<p>​            9）<strong>PPO-Clip</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} J_{P P O 2}^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} \min &\left(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right),\right.\  \\ &\left.\operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right) \end{aligned}</script><p>​                </p>
<p>Min 这个操作符(operator)做的事情是第一项跟第二项里面选比较小的那个。</p>
<p>第二项前面有个 clip 函数，clip 函数的意思是说，</p>
<ul>
<li>在括号里面有三项，如果第一项小于第二项的话，那就输出 $1-\varepsilon$ 。</li>
<li>第一项如果大于第三项的话，那就输出 $1+\varepsilon$。</li>
</ul>
<p>$\varepsilon$ 是一个超参数，你要 tune 的，你可以设成 0.1 或 设 0.2 。</p>
<p><img src="https://s2.loli.net/2022/04/16/M78EQ53ghfvyTO2.png" alt="5.14"></p>
<hr>
<h3 id="4月17日"><a href="#4月17日" class="headerlink" title="4月17日"></a>4月17日</h3><p>实现 PPO算法 玩 Pendulum-v1。</p>
<p><img src="https://s2.loli.net/2022/04/17/ObnDNdgfz4HYpcC.png"  /></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i>强化学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/09/%E5%B0%81%E9%9D%A23-22/" rel="prev" title="封面">
      <i class="fa fa-chevron-left"></i> 封面
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/04/12/DQN%E5%80%92%E7%AB%8B%E6%91%86%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" rel="next" title="DQN倒立摆代码解析">
      DQN倒立摆代码解析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8811%E6%97%A5"><span class="nav-number">1.</span> <span class="nav-text">4月11日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8812%E6%97%A5"><span class="nav-number">2.</span> <span class="nav-text">4月12日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8813%E6%97%A5"><span class="nav-number">3.</span> <span class="nav-text">4月13日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8814%E6%97%A5"><span class="nav-number">4.</span> <span class="nav-text">4月14日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8815%E6%97%A5"><span class="nav-number">5.</span> <span class="nav-text">4月15日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8816%E6%97%A5"><span class="nav-number">6.</span> <span class="nav-text">4月16日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8817%E6%97%A5"><span class="nav-number">7.</span> <span class="nav-text">4月17日</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="P.M.Y"
      src="/images/creator.jpg">
  <p class="site-author-name" itemprop="name">P.M.Y</p>
  <div class="site-description" itemprop="description">记录·分享·进步</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">P.M.Y</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
