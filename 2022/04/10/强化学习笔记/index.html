<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo_icon001.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo_icon001.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="记录自己强化学习学习过程">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记">
<meta property="og:url" content="http://example.com/2022/04/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="动态">
<meta property="og:description" content="记录自己强化学习学习过程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/04/11/VeLkxvf93CpXI4l.jpg">
<meta property="og:image" content="https://s2.loli.net/2022/04/11/sG1vbJLijYZStgD.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/11/Y8dkctwnWX3xJOy.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/APTFzEkv1ZMiGQx.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/SOyXw3Vgd7WCtBk.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/zJUQEgT1XStOHxf.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/riFPYbIwQ7jLWfm.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/kEdlX1IupJPsroq.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/13/IYFwo8DcpELsXnP.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/14/15EzaUgscWky89t.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/14/narPG5kOxpNVwD9.png">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2022/04/14/371ae56e60eaa013.png">
<meta property="og:image" content="https://s1.ax1x.com/2022/04/14/L1kh1P.png">
<meta property="og:image" content="https://img30.360buyimg.com/pop/jfs/t1/151963/30/22549/983753/6257f23eEba2269cf/141f1bcf9aa79f54.png">
<meta property="og:image" content="https://img30.360buyimg.com/pop/jfs/t1/214725/25/17102/632112/6257f21bE89ca862a/774c8fe0fdd93b77.png">
<meta property="og:image" content="https://img30.360buyimg.com/pop/jfs/t1/188468/40/23237/1878038/6257f051Ef267260a/0b049abbd92a7cce.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/u3MjVKWYFRAIiTb.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/Yte1AsnRpvOm3bw.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/pZjodmA9li34EIV.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/8mg9yo1sVQJuDbk.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/16/M78EQ53ghfvyTO2.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/17/ObnDNdgfz4HYpcC.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/18/xZERljQ4V6BwnLk.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/18/3bidn78rJN2HkKw.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/18/1nHLEcAvhJflDOu.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/18/tYpyQ8vnWduIqe1.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/18/YKlXsjvpW3STLge.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/18/8UcKnF321vfCglZ.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/19/lR5OxEh7myNibTt.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/19/bp5Zd3xfEAnsYJz.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/19/GJ58IULaNuQFMyf.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/19/PFIsq5jQR1Ogc7h.png">
<meta property="og:image" content="https://pic.rmb.bdstatic.com/bjh/db7ae6e4b2c3a5d1cafcbdee2fef5b74.png">
<meta property="og:image" content="https://pic.rmb.bdstatic.com/bjh/2872fb7c5072cdfd51f58d020be79b47.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/20/jKGMJOwI5uBi8TZ.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/20/kwxHRvQ6UEIlshf.png">
<meta property="og:image" content="https://pic.rmb.bdstatic.com/bjh/e0beb3014d9953aaf1deac573d782228.png">
<meta property="og:image" content="https://pic.rmb.bdstatic.com/bjh/e5ce498c4f16a558d8e7becbf22005ef.png">
<meta property="og:image" content="https://pic.rmb.bdstatic.com/bjh/5e6b9882c2dae56a41a99a638ec529bb.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/21/STYRBn8igxw9FO4.jpg">
<meta property="article:published_time" content="2022-04-10T09:18:13.000Z">
<meta property="article:modified_time" content="2022-04-21T02:42:05.097Z">
<meta property="article:author" content="P.M.Y">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/04/11/VeLkxvf93CpXI4l.jpg">

<link rel="canonical" href="http://example.com/2022/04/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习笔记 | 动态</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3be792f493e893537d1d13a6c8b3b054";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">动态</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/creator.jpg">
      <meta itemprop="name" content="P.M.Y">
      <meta itemprop="description" content="记录·分享·进步">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="动态">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">

	    
            	<i class="fas fa-thumbtack"></i>
            	<font color=E90A1F>置顶</font>
            	<span class="post-meta-divider">|</span>
            

              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-10 17:18:13" itemprop="dateCreated datePublished" datetime="2022-04-10T17:18:13+08:00">2022-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-21 10:42:05" itemprop="dateModified" datetime="2022-04-21T10:42:05+08:00">2022-04-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="https://s2.loli.net/2022/04/11/VeLkxvf93CpXI4l.jpg" alt="ismail-inceoglu-cauligrx"></p>
<p>记录自己强化学习学习过程</p>
<span id="more"></span>
<h3 id="4月11日"><a href="#4月11日" class="headerlink" title="4月11日"></a>4月11日</h3><ol>
<li><p>reinforcement learning算法的本质就是让agent获得状态转移矩阵</p>
</li>
<li><p>监督学习与强化学习的区别：</p>
<p>监督学习——Learning from teacher</p>
<p>强化学习——Learning from experience</p>
<p>Alpha Go 先进行监督学习，获得不错的效果之后再进行强化学习。</p>
</li>
<li><p>强化学习的困难之处</p>
<p>1）Reward delay</p>
<p>2）Agent的行为会对之后的情况产生影响（需要Agent具备探索能力）</p>
</li>
<li><p>Machine Learning ≈ Looking for a Function</p>
<p>在强化学习中，这里的Function相当于Action = Π(Observation)</p>
</li>
<li><p>强化学习方法</p>
<p>1）model-based(针对未来情况进行预测)</p>
<p>2）model-free (policy-based and/or value-based)</p>
<p>​    例：Alpha Go采用的是 policy-based+value-based+model-based</p>
</li>
<li><p>在一场游戏里面，我们把环境输出的 s 跟演员输出的行为 a ，把 s  跟 a  全部串起来， 叫做一个 <code>Trajectory(轨迹)</code>，如下式所示。</p>
<script type="math/tex; mode=display">
\text { Trajectory } \tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}</script></li>
<li><p>你可以计算每一个轨迹发生的概率。假设现在演员的参数已经被给定了话，就是 $\theta$。根据 $\theta$，你其实可以计算某一个轨迹发生的概率，你可以计算某一个回合里面发生这样子状况的概率。</p>
<script type="math/tex; mode=display">
\begin{aligned} p_{\theta}(\tau) &=p\left(s_{1}\right) p_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) p_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots \ \\ &=p\left(s_{1}\right) \prod_{t=1}^{T} p_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right) \end{aligned}</script></li>
<li><p>你可以根据 $\theta$ 算出某一个轨迹 $\tau$ 出现的概率，接下来计算这个 $\tau$ 的总奖励是多少。总奖励使用这个 $\tau$ 出现的概率进行加权，对所有的 $\tau$ 进行求和，就是期望值。给定一个参数，你会得到的期望值。</p>
<script type="math/tex; mode=display">
\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=E_{\tau \sim p_{\theta}(\tau)}[R(\tau)]</script></li>
<li><p>计算梯度</p>
<script type="math/tex; mode=display">
\nabla \bar{R}_{\theta}=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script></li>
<li><p>强化学习的一下Tips</p>
<p>1）添加baseline</p>
<script type="math/tex; mode=display">
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right)-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right) \\ b \approx E[R(\tau)]</script><p>2）给每个动作合适的分数</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2022/04/11/sG1vbJLijYZStgD.png" alt="4.17"></p>
<p>​    本来的权重是整场游戏的奖励的总和，现在改成从某个时间 t 开始，假设这个动作是在 t 这个时间点所执行       的，从 t 这个时间点一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的。</p>
<p>​    3）折扣回报</p>
<p>​    4）优势函数</p>
<p>​    把 R-b 这一项合起来，我们统称为<code>优势函数(advantage function)</code>， 用 <code>A</code> 来代表优势函数。优势函数取决于 s 和 a，我们就是要计算的是在某一个状态 s 采取某一个动作 a 的时候，优势函数有多大。</p>
<p><img src="https://s2.loli.net/2022/04/11/Y8dkctwnWX3xJOy.png" alt="4.19"></p>
<p>优势函数的意义在于，假设我们在某一个状态 $s_t$  执行某一个动作 $a_t$，相较于其他可能的动作，它有多好。它在意的不是一个绝对的好，而是相对的好，即<code>相对优势(relative advantage)</code>。因为会减掉一个 b，减掉一个 基线， 所以这个东西是相对的好，不是绝对的好。 $A^{\theta}\left(s_{t}, a_{t}\right)$ 通常可以是由一个网络估计出来的，这个网络叫做 critic。</p>
<hr>
<h3 id="4月12日"><a href="#4月12日" class="headerlink" title="4月12日"></a>4月12日</h3><ol>
<li><p>Behavior Cloning的缺点在于无法告诉Machine哪些是重要的，哪些是不重要的。</p>
</li>
<li><p>Two Learning Scenarios:</p>
<p>1）强化学习</p>
<p>2）Learning by demonstration</p>
</li>
<li><p>Actor-Critic算法：</p>
<p>1）Training an Actor</p>
<p>2）Training a Critic</p>
<p>3）Actor + Critic</p>
</li>
<li><p>一个 Critic 无法决定 Action，Critic 的作用是评估一个 actor Π 好不好。（给出最终奖励的期望）</p>
<p>评论家(Critic)是指值函数 $V^{\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。</p>
</li>
</ol>
<hr>
<h3 id="4月13日"><a href="#4月13日" class="headerlink" title="4月13日"></a>4月13日</h3><ol>
<li><p>DQN算法学习</p>
<p>1）有一种 Critic 是 <code>state value function(状态价值函数)</code>。评论家的输出是跟演员有关的，状态的价值其实取      决于你的演员，当演员变的时候，状态价值函数的输出其实也是会跟着改变的。</p>
<p>2）衡量状态价值函数 $V^{\pi}(s)$ 的两种方法：</p>
<p>​      （1）Monte-Carlo(MC)-based</p>
<p>​                在训练的时候， 它就是一个回归问题。网络的输出就是一个值，你希望在输入 $s_a$ 的时候，输出                 的值跟 $G_a$ 越近越好，输入 $s_b$ 的时候，输出的值跟 $G_b$ 越近越好。</p>
<p><img src="https://s2.loli.net/2022/04/13/APTFzEkv1ZMiGQx.png" alt=""></p>
<p>​      （2）Temporal-difference(时序差分)</p>
<p>​                TD-based 的方法不需要把游戏玩到底，只要在游戏的某一个情况，某一个状态 $s_t$ 的时候，采取动                作 $a_t$ 得到奖励$r_t$ ，跳到状态 $s_{t+1}$，就可以使用 TD 的方法。</p>
<p>​                <script type="math/tex">V^{\pi}\left(s_{t}\right)=V^{\pi}\left(s_{t+1}\right)+r_{t}</script></p>
<p><img src="https://s2.loli.net/2022/04/13/SOyXw3Vgd7WCtBk.png" alt=""></p>
<p>3）另一种 Ctitic 是 Q-function，也叫<code>state-action value function(状态-动作价值函数)</code>。状态-动作价值        函数的输入是一个状态、动作对，它的意思是说，在某一个状态采取某一个动作，假设我们都使用演员         $\pi$ ，得到的累积奖励的期望值有多大。</p>
<p>​       推导可得：</p>
<script type="math/tex; mode=display">
\begin{aligned} V^{\pi}(s) &\le Q^{\pi}(s,\pi'(s)) \ \\ &=E\left[r_{t}+V^{\pi}\left(s_{t+1}\right) | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right]\ \\&\le E\left[r_{t}+Q^{\pi}\left(s_{t+1}, \pi^{\prime}\left(s_{t+1}\right)\right) | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\&=E\left[r_{t}+r_{t+1}+V^{\pi}\left(s_{t+2}\right) |s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & \le E\left[r_{t}+r_{t+1}+Q^{\pi}\left(s_{t+2},\pi'(s_{t+2}\right) | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & = E\left[r_{t}+r_{t+1}+r_{t+2}+V^{\pi}\left(s_{t+3}\right) |s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & \le \cdots\ \\ & \le E\left[r_{t}+r_{t+1}+r_{t+2}+\cdots | s_{t}=s, a_{t}=\pi^{\prime}\left(s_{t}\right)\right] \ \\ & = V^{\pi'}(s) \end{aligned}</script><p>​        因此：</p>
<p>​        <script type="math/tex">V^{\pi}(s)\le V^{\pi'}(s)</script></p>
</li>
</ol>
<p>​        4）DQN中用到的一些Tips:</p>
<p>​              （1）目标网络</p>
<p><img src="https://s2.loli.net/2022/04/13/zJUQEgT1XStOHxf.png" alt=""></p>
<p>​            （2）探索(Exploration)</p>
<p><img src="https://s2.loli.net/2022/04/13/riFPYbIwQ7jLWfm.png" alt=""></p>
<p>​            （3）Experience Replay(经验回放)</p>
<p>​                     构建一个 <code>Replay Buffer</code>，用先有某一个策略 $\pi$ 去跟环境做互动，然后它会去收集数据，并利用收                    集到的数据更新参数。</p>
<p><img src="https://s2.loli.net/2022/04/13/kEdlX1IupJPsroq.png" alt="6.18"></p>
<p>​    5）DQN算法总结</p>
<p>​          <img src="https://s2.loli.net/2022/04/13/IYFwo8DcpELsXnP.png" alt="6.19"></p>
<hr>
<h3 id="4月14日"><a href="#4月14日" class="headerlink" title="4月14日"></a>4月14日</h3><ol>
<li><p>DQN进阶技巧</p>
<p>1）Double DQN</p>
<p><img src="https://s2.loli.net/2022/04/14/15EzaUgscWky89t.png" alt=""></p>
<p>2）Dueling DQN</p>
<p><img src="https://s2.loli.net/2022/04/14/narPG5kOxpNVwD9.png" alt="7.6"></p>
<p>3）Prioritized Experience Replay</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/04/14/371ae56e60eaa013.png" alt=""></p>
<p>4）Balance between MC and TD</p>
<p><img src="https://s1.ax1x.com/2022/04/14/L1kh1P.png" alt="L1kh1P.png"></p>
<p>5）Noisy Net</p>
<p><img src="https://img30.360buyimg.com/pop/jfs/t1/151963/30/22549/983753/6257f23eEba2269cf/141f1bcf9aa79f54.png" alt="7.10.png"></p>
<p>6）Distributional Q-function</p>
<p><img src="https://img30.360buyimg.com/pop/jfs/t1/214725/25/17102/632112/6257f21bE89ca862a/774c8fe0fdd93b77.png" alt="7.12.png"></p>
<p>7）Rainbow</p>
<p><img src="https://img30.360buyimg.com/pop/jfs/t1/188468/40/23237/1878038/6257f051Ef267260a/0b049abbd92a7cce.png" alt="7.13.png"></p>
</li>
</ol>
<hr>
<h3 id="4月15日"><a href="#4月15日" class="headerlink" title="4月15日"></a>4月15日</h3><p>实现 DQN算法 玩 MountainCar-v0，并能够保存模型和加载模型。</p>
<p><img src="https://s2.loli.net/2022/04/16/u3MjVKWYFRAIiTb.png" alt=""></p>
<hr>
<h3 id="4月16日"><a href="#4月16日" class="headerlink" title="4月16日"></a>4月16日</h3><ol>
<li><p>PPO算法学习</p>
<p>1）on-policy 和 off-policy</p>
</li>
</ol>
<p>​     如果要学习的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做<code>on-policy(同策略)</code>。</p>
<p>​     如果要学习的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做<code>off-policy(异策略)</code>。</p>
<p> 2）policy gradient 是一个 on-policy 的算法。</p>
<p> 3）<code>近端策略优化(Proximal Policy Optimization，简称 PPO)</code> 是 policy gradient 的一个变形，它是现      在  OpenAI 默认的强化学习算法。</p>
<script type="math/tex; mode=display">
\nabla \bar{R}{\theta}=E{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right]</script><p> 4）重要性采样(Importance Sampling，IS)</p>
<script type="math/tex; mode=display">
E_{x \sim p}[f(x)] \approx \frac{1}{N} \sum_{i=1}^N f(x^i)</script><p>​      期望值 $E_{x \sim p}[f(x)]$ 其实就是 $\int f(x) p(x) dx$，对其做如下的变换：</p>
<script type="math/tex; mode=display">
\int f(x) p(x) d x=\int f(x) \frac{p(x)}{q(x)} q(x) d x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]</script><p>​      就可以写成对 q 里面所采样出来的 $x$ 取期望值，其中 $\frac{p(x)}{q(x)}$ 为<code>重要性权重(importance weight)</code> 。</p>
<p>​      在实现上，p 和 q 不能差太多。因为方差不同：</p>
<script type="math/tex; mode=display">
\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \\
\begin{aligned} \operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2} \ \\ &=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \end{aligned}</script><p><img src="https://s2.loli.net/2022/04/16/Yte1AsnRpvOm3bw.png" alt="5.4"></p>
<p> 5）on-policy  —-&gt;  off-policy</p>
<p><img src="https://s2.loli.net/2022/04/16/pZjodmA9li34EIV.png" alt="5.6"></p>
<script type="math/tex; mode=display">
E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
 =E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(s_{t}, a_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}, a_{t}\right)} A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
 =E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} \frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
 =E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]</script><p>  现在我们得到一个新的目标函数:</p>
<script type="math/tex; mode=display">
J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]</script><p>6）<strong>PPO 是 on-policy 的算法</strong>，<strong>在 PPO 中 $\theta’$ 是 $\theta_{\text{old}}$，即 behavior policy 也是 $\theta$</strong>。如果 $p_{\theta}\left(a_{t} | s_{t}\right)$ 跟 $p_{\theta’}\left(a_{t} | s_{t}\right)$ 这两个分布差太多的话，重要性采样的结果就会不好。这也是PPO要解决的问题。</p>
<p>7）在做 PPO 的时候，所谓的 KL 散度并不是参数的距离，而是动作的距离。</p>
<p>8）<strong>PPO-Penalty</strong></p>
<p>​                    <img src="https://s2.loli.net/2022/04/16/8mg9yo1sVQJuDbk.png" alt="5.9"></p>
<p>​            9）<strong>PPO-Clip</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} J_{P P O 2}^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} \min &\left(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right),\right.\  \\ &\left.\operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right) \end{aligned}</script><p>​                </p>
<p>Min 这个操作符(operator)做的事情是第一项跟第二项里面选比较小的那个。</p>
<p>第二项前面有个 clip 函数，clip 函数的意思是说，</p>
<ul>
<li>在括号里面有三项，如果第一项小于第二项的话，那就输出 $1-\varepsilon$ 。</li>
<li>第一项如果大于第三项的话，那就输出 $1+\varepsilon$。</li>
</ul>
<p>$\varepsilon$ 是一个超参数，你要 tune 的，你可以设成 0.1 或 设 0.2 。</p>
<p><img src="https://s2.loli.net/2022/04/16/M78EQ53ghfvyTO2.png" alt="5.14"></p>
<hr>
<h3 id="4月17日"><a href="#4月17日" class="headerlink" title="4月17日"></a>4月17日</h3><p>实现 PPO算法 玩 Pendulum-v1。</p>
<p><img src="https://s2.loli.net/2022/04/17/ObnDNdgfz4HYpcC.png"  /></p>
<hr>
<h3 id="4月18日"><a href="#4月18日" class="headerlink" title="4月18日"></a>4月18日</h3><ol>
<li><p>Actor-Critic算法学习</p>
<p>1）<code>演员-评论家算法(Actor-Critic Algorithm)</code>是一种结合<code>策略梯度</code>和<code>时序差分学习</code>的强化学习方法，其中：</p>
<ul>
<li>演员(Actor)是指策略函数 $\pi_{\theta}(a|s)$，即学习一个策略来得到尽量高的回报。</li>
<li>评论家(Critic)是指值函数 $V^{\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。</li>
<li>借助于值函数，演员-评论家算法可以进行单步更新参数，不需要等到回合结束才进行更新。</li>
</ul>
<p>2）A2C——Advantage Actor-Critic</p>
<p>​      A3C——Asynchronous Advantage Actor-Critic</p>
<p>3）Q-Learning 使基于价值的方法(value-based)，有两种 critics。</p>
<ul>
<li>第一种 critic 是 $V^{\pi}(s)$，它的意思是说，假设 actor 是 $\pi$，拿 $\pi$ 去跟环境做互动，当我们看到状态 s 的时候，接下来累积奖励 的期望值有多少。</li>
<li>还有一个 critic 是 $Q^{\pi}(s,a)$。$Q^{\pi}(s,a)$ 把 s 跟 a 当作输入，它的意思是说，在状态 s 采取动作 a，接下来都用 actor $\pi$ 来跟环境进行互动，累积奖励的期望值是多少。</li>
<li>$V^{\pi}$ 输入 s，输出一个标量。</li>
<li>$Q^{\pi}$ 输入 s，然后它会给每一个 a 都分配一个 Q value。</li>
<li>你可以用 TD 或 MC 来估计。用 TD 比较稳，用 MC 比较精确。</li>
</ul>
<p>4）A2C (Advantage Actor-Critic)</p>
<p><img src="https://s2.loli.net/2022/04/18/xZERljQ4V6BwnLk.png" alt="9.3"></p>
<p><img src="https://s2.loli.net/2022/04/18/3bidn78rJN2HkKw.png" alt="9.4"></p>
<p><strong>A2C算法流程：</strong></p>
<p><img src="https://s2.loli.net/2022/04/18/1nHLEcAvhJflDOu.png" alt="9.5"></p>
<p>用到的Tips:</p>
<p>（1）<strong>actor (policy) 和 critic (V function) 的输入都是 s，所以它们前面几个层(layer)是可以共享的。</strong></p>
<p>（2）<strong>仍然需要探索(exploration)的机制。</strong></p>
<p>5）A3C （Asynchronous(异步的) Advantage Actor-Critic）</p>
<p><strong>A3C 这个方法就是同时开很多个 worker，那每一个 worker 其实就是一个影分身。那最后这些影分身会把所有的经验，通通集合在一起。</strong></p>
</li>
</ol>
<p>​        <img src="https://s2.loli.net/2022/04/18/tYpyQ8vnWduIqe1.png" alt="9.8"></p>
<p>​        6）<strong>Pathwise Derivative Policy Gradient</strong></p>
<p>​            在Q-Learning算法基础上进行改变。</p>
<p>​            <img src="https://s2.loli.net/2022/04/18/YKlXsjvpW3STLge.png" alt="9.12"></p>
<p>​        <img src="https://s2.loli.net/2022/04/18/8UcKnF321vfCglZ.png" alt="9.13"></p>
<p>​        7） GAN 跟 Actor-Critic 的方法非常类似。</p>
<hr>
<h3 id="4月19日"><a href="#4月19日" class="headerlink" title="4月19日"></a>4月19日</h3><p>实现 A3C算法 玩 Acrobot-v1，并利用matplotlib绘制期望奖励曲线。</p>
<p><img src="https://s2.loli.net/2022/04/19/lR5OxEh7myNibTt.png" alt=""></p>
<p><img src="https://s2.loli.net/2022/04/19/bp5Zd3xfEAnsYJz.png" alt=""></p>
<p>19.DDPG算法学习</p>
<p>​    1）<strong>随机性策略</strong>与<strong>确定性策略</strong></p>
<ul>
<li>对随机性的策略来说，输入某一个状态 s，采取某一个 action 的可能性并不是百分之百，而是有一个概率 P 的，就好像抽奖一样，根据概率随机抽取一个动作。</li>
<li>而对于确定性的策略来说，它没有概率的影响。当神经网络的参数固定下来了之后，输入同样的 state，必然输出同样的 action，这就是确定性的策略。</li>
</ul>
<p><img src="https://s2.loli.net/2022/04/19/GJ58IULaNuQFMyf.png" alt="12.3"></p>
<p>​    *tanh 的图像的像右边这样子，它的作用就是把输出限制到 [-1,1] 之间。</p>
<p>​    2）DDPG 的特点可以从它的名字（<strong>Deep Deterministic Policy Gradient</strong>）当中拆解出来，拆解成 Deep、Deterministic 和 Policy Gradient。</p>
<ul>
<li>Deep 是因为用了神经网络；</li>
<li>Deterministic 表示 DDPG 输出的是一个确定性的动作，可以用于连续动作的一个环境；</li>
<li>Policy Gradient 代表的是它用到的是策略网络。REINFORCE 算法每隔一个 episode 就更新一次，但 DDPG 网络是每个 step 都会更新一次 policy 网络，也就是说它是一个<strong>单步更新</strong>的 policy 网络。</li>
</ul>
<p><img src="https://s2.loli.net/2022/04/19/PFIsq5jQR1Ogc7h.png" alt="12.5"></p>
<p>​    <strong>提出 DDPG 是为了让 DQN 可以扩展到连续的动作空间</strong>。</p>
<ul>
<li>DDPG 直接在 DQN 基础上加了一个<strong>策略网络</strong>来直接输出动作值，所以 DDPG 需要<strong>一边学习 Q 网络，一边学习策略网络</strong>。</li>
<li>Q 网络的参数用 $w$ 来表示。策略网络的参数用 $\theta$ 来表示。</li>
<li>我们称这样的结构为 <code>Actor-Critic</code> 的结构。</li>
</ul>
<p><img src="https://pic.rmb.bdstatic.com/bjh/db7ae6e4b2c3a5d1cafcbdee2fef5b74.png" alt="12.7.png"></p>
<p>​    <img src="https://pic.rmb.bdstatic.com/bjh/2872fb7c5072cdfd51f58d020be79b47.png" alt="12.8.png"></p>
<ul>
<li>DDPG 有四个网络，<strong>策略网络的 target 网络</strong> 和 <strong>Q 网络的 target 网络</strong>就是颜色比较深的这两个，它只是为了让计算 Q_target 的时候能够更稳定一点而已。<strong>因为这两个网络也是固定一段时间的参数之后再跟评估网络同步一下最新的参数。</strong></li>
<li>因为 DDPG 使用了经验回放这个技巧，所以 DDPG 是一个 <code>off-policy</code> 的算法。</li>
</ul>
<p>​    3）<strong>TD3</strong>（Twin Delayed DDPG 双延迟深度确定性策略梯度）</p>
<ul>
<li><p><strong>截断的双 Q 学习(Clipped Dobule Q-learning)</strong> 。TD3 学习两个 Q-function（因此名字中有 “twin”）。TD3 通过最小化均方误差来同时学习两个 Q-function：$Q_{\phi_1}$ 和 $Q_{\phi_2}$。两个 Q-function 都使用一个目标，两个 Q-function 中给出较小的值会被作为如下的 Q-target：</p>
<script type="math/tex; mode=display">
y\left(r, s^{\prime}, d\right)=r+\gamma(1-d) \min_{i=1,2} Q_{\phi_{i, t a r g}}\left(s^{\prime}, a_{T D 3}\left(s^{\prime}\right)\right)</script></li>
<li><p><strong>延迟的策略更新(“Delayed” Policy Updates)</strong> 。相关实验结果表明，同步训练动作网络和评价网络，却不使用目标网络，会导致训练过程不稳定；但是仅固定动作网络时，评价网络往往能够收敛到正确的结果。因此 TD3 算法以较低的频率更新动作网络，较高频率更新评价网络，通常每更新两次评价网络就更新一次策略。</p>
</li>
<li><p><strong>目标策略平滑(Target Policy smoothing)</strong> 。TD3 引入了 smoothing 的思想。TD3 在目标动作中加入噪音，通过平滑 Q 沿动作的变化，使策略更难利用 Q 函数的误差。</p>
<p>目标策略平滑化(一种正则化方法)的工作原理如下：</p>
<script type="math/tex; mode=display">
a_{T D 3}\left(s^{\prime}\right)=\operatorname{clip}\left(\mu_{\theta, t a r g}\left(s^{\prime}\right)+\operatorname{clip}(\epsilon,-c, c), a_{\text {low }}, a_{\text {high }}\right)</script><p>其中 $\epsilon$ 本质上是一个噪声，是从正态分布中取样得到的，即 $\epsilon \sim N(0,\sigma)$。</p>
</li>
</ul>
<hr>
<h3 id="4月20日"><a href="#4月20日" class="headerlink" title="4月20日"></a>4月20日</h3><p>研读Openai gym Wiki中大佬们的代码。</p>
<p><img src="https://s2.loli.net/2022/04/20/jKGMJOwI5uBi8TZ.png" alt=""></p>
<ol>
<li><p>深度强化学习算法伪代码汇总：</p>
<p>1）DQN</p>
</li>
</ol>
<p><img src="https://s2.loli.net/2022/04/20/kwxHRvQ6UEIlshf.png" alt=""></p>
<p>​    2）PPO</p>
<p>​    <img src="https://pic.rmb.bdstatic.com/bjh/e0beb3014d9953aaf1deac573d782228.png" alt="BWQ@UA}78G_T[VI0P_EC]8B.png"></p>
<p>​    3）A3C</p>
<p>​    <img src="https://pic.rmb.bdstatic.com/bjh/e5ce498c4f16a558d8e7becbf22005ef.png" alt=""></p>
<p>​    4）DDPG</p>
<p>​    <img src="https://pic.rmb.bdstatic.com/bjh/5e6b9882c2dae56a41a99a638ec529bb.png" alt="@C@P8TJAY{3F5JW8UR9YYKE.png"></p>
<hr>
<h3 id="4月21日"><a href="#4月21日" class="headerlink" title="4月21日"></a>4月21日</h3><p>把深度强化学习主要算法过了一遍，也能跑通gym上的环境，发现自己并不能根据算法进行代码的复现，而且对一些深度学习网络模型也不是很了解，决定从今天开始啃《动手学深度学习》（Pytorch版本）</p>
<p><img src="https://s2.loli.net/2022/04/21/STYRBn8igxw9FO4.jpg" alt="v2-5642db38e1f5d74b81ae90c55d549553_1440w"></p>
<p><a target="_blank" rel="noopener" href="https://panmingyan.github.io/2022/04/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#more">我的深度学习记录</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i>强化学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/09/%E5%B0%81%E9%9D%A23-22/" rel="prev" title="封面">
      <i class="fa fa-chevron-left"></i> 封面
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/04/12/DQN%E5%80%92%E7%AB%8B%E6%91%86%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" rel="next" title="DQN倒立摆代码解析">
      DQN倒立摆代码解析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8811%E6%97%A5"><span class="nav-number">1.</span> <span class="nav-text">4月11日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8812%E6%97%A5"><span class="nav-number">2.</span> <span class="nav-text">4月12日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8813%E6%97%A5"><span class="nav-number">3.</span> <span class="nav-text">4月13日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8814%E6%97%A5"><span class="nav-number">4.</span> <span class="nav-text">4月14日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8815%E6%97%A5"><span class="nav-number">5.</span> <span class="nav-text">4月15日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8816%E6%97%A5"><span class="nav-number">6.</span> <span class="nav-text">4月16日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8817%E6%97%A5"><span class="nav-number">7.</span> <span class="nav-text">4月17日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8818%E6%97%A5"><span class="nav-number">8.</span> <span class="nav-text">4月18日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8819%E6%97%A5"><span class="nav-number">9.</span> <span class="nav-text">4月19日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8820%E6%97%A5"><span class="nav-number">10.</span> <span class="nav-text">4月20日</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E6%9C%8821%E6%97%A5"><span class="nav-number">11.</span> <span class="nav-text">4月21日</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="P.M.Y"
      src="/images/creator.jpg">
  <p class="site-author-name" itemprop="name">P.M.Y</p>
  <div class="site-description" itemprop="description">记录·分享·进步</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">P.M.Y</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
